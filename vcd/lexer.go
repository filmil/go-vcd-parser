package vcd

import (
	"fmt"

	"github.com/alecthomas/participle/v2/lexer"
	"golang.org/x/text/cases"
	"golang.org/x/text/language"
)

// See: https://github.com/google/re2/wiki/Syntax
const (
	BinstringPattern  = `[bB]([10xXzZ])+`
	FloatPattern      = `[+-]?([0-9]*\.?[0-9]+|[0-9]+\.?[0-9]*)([eE][+-]?[0-9]+)?` // Generated by Gemini.
	RealStringPattern = `[r|R]` + FloatPattern
	IntPattern        = `[+-]?[0-9]+`
	StringPattern     = `\w+`
	TimestampPattern  = `#\d+`
	WhitespacePattern = `\s+`
	IdentifierPattern = `[a-zA-Z_][a-zA-Z0-9_]*`
)

// IntoRule converts a SimpleRule into a (complex) Rule.
func IntoRule(rules []lexer.SimpleRule) []lexer.Rule {
	var ret []lexer.Rule
	for _, r := range rules {
		newRule := lexer.Rule{
			Name:    r.Name,
			Pattern: r.Pattern,
			Action:  nil,
		}
		ret = append(ret, newRule)
	}
	return ret
}

func GenKeywordTokens() []lexer.SimpleRule {
	var caser = cases.Title(language.AmericanEnglish)
	var keywords = []string{
		// These are implemented via special tokenizer states.
		//"comment",
		//"date",
		"enddefinitions",
		"scope",
		"timescale",
		"upscope",
		"var",
		//"version",
		"dumpall",
		"dumpon",
		"dumpoff",
		"dumpvars",
		"end",
	}

	var ret []lexer.SimpleRule
	for _, kw := range keywords {
		ret = append(ret, lexer.SimpleRule{
			Name: fmt.Sprintf("Kw%v", caser.String(kw)),
			// Don't forget to escape your `$`
			Pattern: fmt.Sprintf(`\$%v`, kw),
		})
	}
	return ret
}

var rules = []lexer.SimpleRule{
	{
		Name:    "Timestamp",
		Pattern: TimestampPattern,
	},
	{
		Name:    "Binstring",
		Pattern: BinstringPattern,
	},
	{
		Name:    "Identifier",
		Pattern: IdentifierPattern,
	},
	{
		Name:    "RealString",
		Pattern: RealStringPattern,
	},
	{
		Name:    "Int",
		Pattern: IntPattern,
	},
	{
		Name:    "Float",
		Pattern: FloatPattern,
	},
	{
		Name:    "IdCode",
		Pattern: AnyWordPattern,
	},
	{
		Name:    "String",
		Pattern: StringPattern,
	},
	{
		Name:    "ws",
		Pattern: WhitespacePattern,
	},
}

// SimpleRules returns keywords plus basic token types.
func SimpleRules(additions []lexer.Rule) []lexer.Rule {
	ret := append(IntoRule(GenKeywordTokens()), additions...)
	ret = append(ret, IntoRule(rules)...)
	//fmt.Printf("ret: %+v", ret)
	return ret
}

const (
	// Any string that consists of non-whitespace.
	AnyWordPattern = `\S+`
)

// anyWordsEndingWithKwEnd is a set of lexical rules that accept a sequence of
// any words, separated by whitespace, until `$end` is encountered.
var anyWordsEndingWithKwEnd = []lexer.Rule{
	// Allows matching $end in $end, but not in $enddefinition.
	{Name: "KwEndSpecial", Pattern: `\$end(\s+|$)`, Action: lexer.Pop()},
	{Name: "AnyNonspace", Pattern: AnyWordPattern, Action: nil},
	{Name: "ws", Pattern: `\s+`, Action: nil},
}

// NewLexer returns a built lexer that produces a valid stream of VCD tokens.
//
// The lexer is lightly stateful to allow date and comment keywords and such.
func NewLexer() *lexer.StatefulDefinition {
	return lexer.MustStateful(lexer.Rules{
		"Root": SimpleRules([]lexer.Rule{
			{Name: "KwDate", Pattern: `\$date`, Action: lexer.Push("DateTokens")},
			{Name: "KwComment", Pattern: `\$comment`, Action: lexer.Push("CommentTokens")},
			{Name: "KwVersion", Pattern: `\$version`, Action: lexer.Push("VersionTokens")},
		}),
		"DateTokens": anyWordsEndingWithKwEnd,
		// Is this unnecessary?
		"CommentTokens": {lexer.Include("DateTokens")},
		"VersionTokens": {lexer.Include("DateTokens")},
	})
}
